# -*- coding: utf-8 -*-
"""Machine Learning Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-PEoPW3ZLzsqjKSEdmVYEcilqwDApLoe

# Import Library
"""

!pip install pmdarima
!pip install statsmodels

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

from sklearn import preprocessing
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
import pmdarima as pm
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_samples, silhouette_score
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error
from yellowbrick.cluster import KElbowVisualizer
from itertools import product

"""# Import Data"""

df_customer = pd.read_csv('customer.csv', sep= ';')
df_product = pd.read_csv('product.csv', sep= ';')
df_store = pd.read_csv('store.csv', sep= ';')
df_transaction = pd.read_csv('transaction.csv', sep= ';')

"""# Data Preparation"""

df_transaction.info()

df_customer.info()

df_product.info()

df_store.info()

df1 = pd.merge(df_transaction, df_customer, on='CustomerID', how='inner')
df2 = pd.merge(df1, df_store, on = 'StoreID', how = 'inner')
df_final = pd.merge(df2, df_product, on = 'ProductID', how = 'inner')
df_final.head()

df_final.info()

(df_final['Price_x']/df_final['Price_y']).value_counts()

"""Kolom Price_x dan Price_y memiliki nilai yang sama pada setiap kolomnya sehingga dapat didrop salah satunya"""

df_final.drop(columns ='Price_y', inplace = True)

df_final.isna().sum()

df_final.duplicated().sum()

#drop missing values karena tidak signifikan jumlahnya
df_final.dropna(inplace = True)
df_final.isna().sum()

df_final.info()

df_final.Date

"""### Change Data Type of Irrelevant Data Types"""

df_final['Date'] = pd.to_datetime(df_final['Date'], format='%d/%m/%Y')
df_final['Longitude'] = df_final['Longitude'].apply(lambda x: x.replace(',','.')).astype(float)
df_final['Latitude'] = df_final['Latitude'].apply(lambda x: x.replace(',','.')).astype(float)

df_final.head()

df_final.info()

"""# Time Series Regression Analysis"""

# Visualize Data
df_regression = df_final.groupby('Date').agg({'Qty':'sum'})
df_regression.plot(figsize=(10,3))

#Split Data Train & Data Test
print(df_regression.shape)
test_size = round(df_regression.shape[0] * 0.15)
train=df_regression.iloc[:-1*(test_size)]
test=df_regression.iloc[-1*(test_size):]
print(train.shape,test.shape)

plt.figure(figsize=(10,3))
sns.lineplot(data=train, x=train.index, y=train['Qty'])
sns.lineplot(data=test, color="green", x=test.index, y=test['Qty'])
plt.show()

"""## Data Stationary Check"""

def adf_test(dataset):
     df_test = adfuller(dataset, autolag = 'AIC')
     print("1. ADF : ",df_test[0])
     print("2. P-Value : ", df_test[1])
     print("3. Num Of Lags : ", df_test[2])
     print("4. Num Of Observations Used For ADF Regression:", df_test[3])
     print("5. Critical Values :")
     for key, val in df_test[4].items():
         print("\t",key, ": ", val)
adf_test(df_regression)

"""###### P-Value < 0.05 shows that the data is stationary and can be used in time series analysis with ARIMA"""

# ACF and PACF plots to determine p and q values
fig, ax = plt.subplots(1, 2, figsize=(10, 3))
plot_acf(df_regression.diff().dropna(), lags=40, ax=ax[0])
plot_pacf(df_regression.diff().dropna(), lags=40, ax=ax[1])
plt.show()

"""###### The Autocorrelation graph (ACF) shows that the p order is 2 because the first and second lag is significantly out of the significant limit, <br> meanwhile the Partial Autocorrelation graph (PCF) shows that the q order is 3 due to the significant correlation of the first until third lag.

## Modelling

### Auto-fit ARIMA
"""

#auto-fit ARIMA
af_arima = pm.auto_arima(train, stepwise=False, seasonal=False)
af_arima

"""### Hyperparameter Tuning"""

# Make p, d, q list
p = range(0, 4)  # 0-3
d = range(0, 4)  # 0-3
q = range(0, 4)  # 0-3
# make p, d, dan q combination using product function
pdq = list(product(p, d, q))
print(pdq)

# Make list to store AIC scores
aic_scores = []
# find the optimal p, d, q
for param in pdq:
    # Melakukan fitting ARIMA model
    model = ARIMA(df_regression, order=param)
    model_fit = model.fit()
    # Menambahkan aic score ke list
    aic_scores.append({'par': param, 'aic': model_fit.aic})

# Memilih aic score terkecil
best_aic = min(aic_scores, key=lambda x: x['aic'])
print(best_aic)

#Hyperparameter tuning
model_ht = ARIMA(train, order=best_aic['par'])
model_fit_ht = model_ht.fit()

"""###  Manual Hyperparameter Tuning"""

#Trial and error tuning
model_manualHt = ARIMA(train, order=(40,2,2))
model_fit_manualHt = model_manualHt.fit()

"""### Plot Forecasting"""

#plot forecasting
forecast_manualHt = model_fit_manualHt.forecast(len(test))
forecast_ht = model_fit_ht.forecast(len(test))
forecast_af = af_arima.predict(len(test))

df_plot = df_regression.iloc[-100:]

df_plot['forecast_manual'] = [None]*(len(df_plot)-len(forecast_manualHt)) + list(forecast_manualHt)
df_plot['forecast_hyper'] = [None]*(len(df_plot)-len(forecast_ht)) + list(forecast_ht)
df_plot['forecast_auto'] = [None]*(len(df_plot)-len(forecast_af)) + list(forecast_af)

df_plot.plot(figsize=(10, 3))
plt.show()

"""### Metrics Evaluation"""

#Manual parameter tuning metrics

mae = mean_absolute_error(test, forecast_manualHt)
mape = mean_absolute_percentage_error(test, forecast_manualHt)
rmse = np.sqrt(mean_squared_error(test, forecast_manualHt))

print(f'Mean Absolute Error - manual            : {round(mae,4)}')
print(f'Mean Absolute Percentage Error - manual : {round(mape,4)}')
print(f'Root Mean Square Error - manual         : {round(rmse,4)}')

#Hyperparameter tuning metrics

mae = mean_absolute_error(test, forecast_ht)
mape = mean_absolute_percentage_error(test, forecast_ht)
rmse = np.sqrt(mean_squared_error(test, forecast_ht))

print(f'Mean Absolute Error - hyper            : {round(mae,4)}')
print(f'Mean Absolute Percentage Error - hyper : {round(mape,4)}')
print(f'Root Mean Square Error - hyper         : {round(rmse,4)}')

#Auto-fit ARIMA metrics

mae = mean_absolute_error(test, forecast_af)
mape = mean_absolute_percentage_error(test, forecast_af)
rmse = np.sqrt(mean_squared_error(test, forecast_af))

print(f'Mean Absolute Error - auto            : {round(mae,4)}')
print(f'Mean Absolute Percentage Error - auto : {round(mape,4)}')
print(f'Root Mean Square Error - auto         : {round(rmse,4)}')

"""###### Manual Hyperparameter Tuning with order (40,2,2) shows the best evaluation metrics.

## Forecast Quantity Sales with The Best Parameter
"""

#Apply model to forecast data
model = ARIMA(df_regression, order=(40, 2, 2))
model_fit = model.fit()
forecast = model_fit.forecast(steps=31)

df_regression

forecast

#Plot forecasting
plt.figure(figsize=(13,3))
plt.plot(df_regression)
plt.plot(forecast,color='green')
plt.title('Quantity Sales Forecasting')
plt.show()

forecast.describe()

"""##### **From the forecast, the average quantity sales in January 2023 is 44.339338 or up rounded to around 44 pcs/day.**

# Clustering
"""

df_final.head()

df_precluster = df_final.groupby('CustomerID').agg({'TransactionID':'count',
                                                   'Qty':'sum',
                                                   'TotalAmount':'sum'}).reset_index()
df_precluster

df_precluster.info()

df_cluster = df_precluster.drop(columns = ['CustomerID'])
df_cluster.head()

df_cluster.info()

df_cluster.isna().sum()

#Standarisasi dataset
X = df_cluster.values
X_std = StandardScaler().fit_transform(X)
df_std = pd.DataFrame(data=X_std,columns=df_cluster.columns)
df_std.isna().sum()

#Normalisasi dataset dengan minmaxscaler
X_norm = MinMaxScaler().fit_transform(X)
X_norm

# Normalisasi dataset dengan preprocessing sklearn
X_norm2 = preprocessing.normalize(df_cluster)
X_norm2

X_std

df_std

wcss= []
for n in range (1,11):
    model1 = KMeans(n_clusters=n, init='k-means++', n_init = 10, max_iter=100, tol =0.0001, random_state = 100)
    model1.fit(X_std)
    wcss.append(model1.inertia_)
print(wcss)

plt.figure(figsize=(7,4))
plt.plot(list(range(1,11)), wcss, color = 'blue', marker = 'o', linewidth=2, markersize=12, markerfacecolor= 'm',
         markeredgecolor= 'm')
plt.title('WCSS vs Number of Cluster')
plt.xlabel('Number of Cluster')
plt.ylabel('WCSS')
plt.xticks(list(range(1,11)))
plt.show()

#Elbow Method with yellowbrick library
visualizer = KElbowVisualizer(model1, k=(2,10))
visualizer.fit(X_std)
visualizer.show()

K = range(2,8)
fits=[]
score=[]

for k in K:
    model = KMeans(n_clusters = k, random_state = 0, n_init= 'auto').fit(X_std)
    fits.append(model)
    score.append(silhouette_score(X_std, model.labels_, metric='euclidean'))

sns.lineplot(x = K, y = score)

"""### Cluster terbaik (k) terdapat pada 4 cluster"""

# Kmeans n_cluster = 4
#Clustering Kmeans
kmeans_4 = KMeans(n_clusters=4,init='k-means++',max_iter=300,n_init=10,random_state=100)
kmeans_4.fit(X_std)

# Masukin cluster ke dataset
df_cluster['cluster'] = kmeans_4.labels_
df_cluster.head()

plt.figure(figsize=(6,6))
sns.pairplot(data=df_cluster,hue='cluster',palette='Set1')
plt.show()

df_cluster['CustomerID'] = df_precluster['CustomerID']
df_cluster_mean = df_cluster.groupby('cluster').agg({'CustomerID':'count','TransactionID':'mean','Qty':'mean','TotalAmount':'mean'})
df_cluster_mean.sort_values('CustomerID', ascending = False)

"""### **Summary**
Notes: Karakteristik metrik: transaksi, kuantitas, total jumlah
<br>
* **Cluster 0** <br>
    Kluster ini memiliki jumlah pelanggan paling sedikit. Pelanggan dalam kelompok ini memiliki karakteristik yang menduduki peringkat tertinggi dalam setiap metrik.
 <br> **Rekomendasi**:
  1. Menawarkan program loyalitas (poin) untuk mempertahankan tingkat transaksi.
  2. Melakukan survei kepuasan pelanggan.
  3. Membuat produk atau paket premium dengan harga yang lebih tinggi
  4. Menawarkan program afiliasi yang tersedia untuk semua orang

* **Cluster 1** <br>
    Kluster ini terdiri dari pelanggan dengan peringkat terendah dalam setiap metrik.
<br> **Rekomendasi**:
  1. Menawarkan Bundling Produk atau Penawaran Diskon Volume
  2. Melakukan survei untuk mengidentifikasi potensi pengembangan produk.
  3. Menyiapkan tim dukungan pelanggan yang responsif dan berpengetahuan untuk membantu pelanggan dengan pertanyaan atau masalah yang mereka alami.
  4. Membuat section FAQ secara online, artikel tutorial, atau forum komunitas tempat pelanggan dapat berinteraksi dan saling berbagi informasi.
  
* **Cluster 2** <br>
    Pelanggan dalam kelompok ini memiliki karakteristik yang menduduki peringkat kedua tertinggi dalam setiap metrik.
<br> **Rekomendasi**:
    1. Fokus melakukan up-selling
    2. Mengadakan program loyalitas
    3. Mengadakan umpan balik secara teratur dari pelanggan untuk memahami kebutuhan dan preferensi mereka.

* **Cluster 3** <br>
    Kluster ini memiliki jumlah pelanggan terbanyak. Para pelanggan dalam kelompok ini memiliki karakteristik yang menempati posisi ketiga tertinggi.
<br> **Rekomendasi**:
    1. Mengadakan program loyalitas atau memberikan penghargaan kepada pelanggan yang setia sebagai bentuk pengakuan dan apresiasi atas bisnis mereka.
    2. Menyelenggarakan survei secara berkala untuk lebih memahami minat utama dari pelanggan.
    3. Tingkatkan komunikasi dengan pelanggan melalui email, pesan teks, atau media sosial untuk tetap terhubung.
"""